server.port=8000
spring.threads.virtual.enabled=true

#spring.ai.ollama.base-url=localhost:11434
#Default url is localhost:11434. So no need to set.

#spring.ai.ollama.embedding.enabled=true\
#It is enabled by default.

spring.ai.ollama.embedding.options.model=llama2
#Default model is mistral. But we are running llama2. So changed here.

